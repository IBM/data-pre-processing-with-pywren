{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\"> Face Recognition Deep Learning with PyWren over IBM Cloud Functions</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains steps and code to demonstrate how serverless computing can provide great benefit for AI data preprocessing. We demonstrate Face Recognition deep learning over Watson Machine Learning service, while letting IBM Cloud Function to do the data preparation phase. As we will show this makes an entire process up to 50 times faster comparing to running the same code without leveraging serverless computing.\n",
    "\n",
    "Our notebook is based on a blog <a href=\"https://hackernoon.com/building-a-facial-recognition-pipeline-with-deep-learning-in-tensorflow-66e7645015b8\" target=\"_blank\" rel=\"noopener no referrer\">Building a Facial Recognition Pipeline with Deep Learning in Tensorflow </a> written by Cole Murray who kindly allowed us to use code and text from his blog.\n",
    "\n",
    "This notebook introduces commands for getting data, training_definition persistance to Watson Machine Learning repository, model training, deployment and scoring.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses \n",
    "\n",
    "- Python 3 \n",
    "- <a href=\"https://dataplatform.cloud.ibm.com/docs/content/analyze-data/environments-parent.html\" target=\"_blank\" rel=\"noopener no referrer\">Watson Studio environments.</a>\n",
    "- IBM Cloud Functions\n",
    "- <a href=\"https://github.com/pywren/pywren-ibm-cloud\" target=\"_blank\" rel=\"noopener no referrer\">PyWren for IBM Cloud</a>\n",
    "\n",
    "\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "In this notebook, you will learn how to:\n",
    "\n",
    "-  Work with Watson Machine Learning experiments to train Deep Learning models (Tensorflow)\n",
    "-  Save trained models in the Watson Machine Learning repository\n",
    "-  Deploy a trained model online and score\n",
    "-  How IBM Cloud Functions can be used for data preparation phase\n",
    "-  Value of PyWren for IBM Cloud\n",
    "\n",
    "\n",
    "## Contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## <span style=\"color:blue\">1. Set up related IBM Cloud Services</span>\n",
    "\n",
    "Before you use the sample code in this notebook, you must setup Watson Machine Learning Service, IBM Cloud Object Storage and IBM Cloud Functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create Watson Machine Learning Service\n",
    "\n",
    "Create a <a href=\"https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance is <a href=\"https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create IBM Cloud Object Storage\n",
    "\n",
    "Create a <a href=\"https://console.bluemix.net/catalog/infrastructure/cloud-object-storage\" target=\"_blank\" rel=\"noopener no referrer\">Cloud Object Storage (COS)</a> instance (a lite plan is offered and information about how to order storage is <a href=\"https://console.bluemix.net/docs/services/cloud-object-storage/basics/order-storage.html#order-storage\" target=\"_blank\" rel=\"noopener no referrer\">here</a>). <br/>**Note: When using Watson Studio, you already have a COS instance associated with the project you are running the notebook in.**\n",
    "\n",
    "- Create new credentials with HMAC: \n",
    "    - Go to your COS dashboard.\n",
    "    - In the **Service credentials** tab, click **New Credential+**.\n",
    "    - Add the inline configuration parameter: {\"HMAC\":true}, click **Add**. (For more information, see <a href=\"https://console.bluemix.net/docs/services/cloud-object-storage/hmac/credentials.html#using-hmac-credentials\" target=\"_blank\" rel=\"noopener no referrer\">HMAC</a>.)\n",
    "\n",
    "    This configuration parameter adds the following section to the instance credentials, (for use later in this notebook):\n",
    "    ```\n",
    "      \"cos_hmac_keys\": {\n",
    "            \"access_key_id\": \"-------\",\n",
    "            \"secret_access_key\": \"-------\"\n",
    "       }\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create IBM Cloud Functions account\n",
    "Setup IBM Cloud Functions account as described here. Please follow all the steps and make sure you can run \"Hello World\" example based on Python code. This will assure your Cloud Functions service is running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> 2. Dependencies installation </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the needed libraries for the Face Recognition. \n",
    "\"dlib\" dependency need to be installed via new environment. Create new environment based on Python 3.5 and add dependency in the customizaion section, as follows\n",
    "\n",
    "    dependencies:\n",
    "     - dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!curl -fsSL \"https://git.io/fhe9X\" | sh\n",
    "try:\n",
    "    import pywren_ibm_cloud as pywren\n",
    "except:\n",
    "    !curl -fsSL \"https://git.io/fhe9X\" | sh\n",
    "    import pywren_ibm_cloud as pywren\n",
    "try:\n",
    "    import cv2\n",
    "except:\n",
    "    !pip install --user opencv-contrib-python\n",
    "try:\n",
    "    from openface.align_dlib import AlignDlib\n",
    "except:    \n",
    "    !git clone https://github.com/cmusatyalab/openface.git\n",
    "    !cd openface ; python setup.py install\n",
    "\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">3. Configuration </span>\n",
    "This section explains how to configure services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 COS Connection\n",
    "You need obtain both credentials to the Cloud Functions and COS.\n",
    "\n",
    "You can find COS credentials in your COS instance dashboard under the Service credentials tab.\n",
    "Note: the HMAC key, described in set up the environment is included in these credentials.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_credentials = {\n",
    "  \"apikey\": \"***\",\n",
    "  \"cos_hmac_keys\": {\n",
    "    \"access_key_id\": \"***\",\n",
    "    \"secret_access_key\": \"***\"\n",
    "  },\n",
    "  \"endpoints\": \"https://cos-service.bluemix.net/endpoints\",\n",
    "  \"iam_apikey_description\": \"Auto generated apikey during resource-key operation for Instance - crn:v1:bluemix:public:cloud-object-storage:global:a/07a95aa44e6124e8b320b70cf88033fa:876e5285-4bef-4cf3-a89b-595e19648c7c::\",\n",
    "  \"iam_apikey_name\": \"auto-generated-apikey-19a79dae-6a58-4b4f-878f-6839b711523f\",\n",
    "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n",
    "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/07a95aa44e6124e8b320b70cf88033fa::serviceid:ServiceId-3f2cccee-61ec-4147-8732-9f58479ba26a\",\n",
    "  \"resource_instance_id\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/07a95aa44e6124e8b320b70cf88033fa:876e5285-4bef-4cf3-a89b-595e19648c7c::\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the endpoint.\n",
    "\n",
    "To do this, go to the **Endpoint** tab in the COS instance's dashboard to get the endpoint information, then enter it in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define endpoint information.\n",
    "service_endpoint = 'https://s3-api.us-geo.objectstorage.softlayer.net'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also need the IBM Cloud authorization endpoint to be able to create COS resource object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the authorization endpoint.\n",
    "auth_endpoint = 'https://iam.bluemix.net/oidc/token'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the boto library. This library allows Python developers to manage Cloud Object Storage (COS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:** If `ibm_boto3` is not preinstalled in you environment, run the following command to install it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the command if ibm_boto3 is not installed.\n",
    "%!pip install ibm-cos-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the boto library.\n",
    "import ibm_boto3\n",
    "from ibm_botocore.client import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Boto resource to be able to write data to COS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a COS resource.\n",
    "cos = ibm_boto3.resource('s3',\n",
    "                         ibm_api_key_id=cos_credentials['apikey'],\n",
    "                         ibm_service_instance_id=cos_credentials['resource_instance_id'],\n",
    "                         ibm_auth_endpoint=auth_endpoint,\n",
    "                         config=Config(signature_version='oauth'),\n",
    "                         endpoint_url=service_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 IBM Cloud Functions setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain api key and endpoint to the IBM Cloud Functions service. Navigate the \"API Key\" menu and copy namespace, host and key. Make sure to add \"https://\" to the host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "          'ibm_cf':  {'endpoint': '<IBM Cloud Functions Endpoint>', \n",
    "                      'namespace': '<NAMESPACE>', \n",
    "                      'api_key': '<API KEY>'}, \n",
    "          'ibm_cos': {'endpoint': '<IBM Cloud Object Storage Endpoint>', \n",
    "                      'api_key' : '<API KEY>'},\n",
    "           'pywren' : {'storage_bucket' : '<IBM COS BUCKET>'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyWren engine requires it's server side component to be deplpoyed in advane. This step creates a new IBM Cloud Function function with PyWren server side runtime. This action will be used internally by PyWren during execution phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywren_ibm_cloud.deployutil import clone_runtime\n",
    "clone_runtime('ibmfunctions/pywren-dlib-runtime:3.5', config, 'pywren-ibm-cloud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">4. Preprocessing Data using Dlib and Docker</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Preparing the Data\n",
    "\n",
    "You’ll use the LFW (Labeled Faces in the Wild) dataset as training data. Below are instructions how you can upload this dataset into your private COS bucket. Alternatively you may replace this with your dataset by following the same structure.\n",
    "\n",
    "\n",
    "     Directory Structure\n",
    "     ├── Tyra_Banks\n",
    "     │ ├── Tyra_Banks_0001.jpg\n",
    "     │ └── Tyra_Banks_0002.jpg\n",
    "     ├── Tyron_Garner\n",
    "     │ ├── Tyron_Garner_0001.jpg\n",
    "     │ └── Tyron_Garner_0002.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a COS bucket, which you will use to store the input data.\n",
    "\n",
    "**Note:** The bucket names must be unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a specific bucket in mind, you can set it here, otherwise leave this blank, and one will be generated\n",
    "BUCKET = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not BUCKET:\n",
    "    bucket_uid = str(uuid4())\n",
    "    BUCKET = 'face-recognition-' + bucket_uid\n",
    "\n",
    "if not cos.Bucket(BUCKET) in cos.buckets.all():\n",
    "    print('Creating bucket \"{}\"...'.format(BUCKET))\n",
    "    try:\n",
    "        cos.create_bucket(Bucket=BUCKET)\n",
    "    except ibm_boto3.exceptions.ibm_botocore.client.ClientError as e:\n",
    "    print('Error: {}.'.format(e.response['Error']['Message']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have at least 1 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display a list of created buckets.\n",
    "print(list(cos.buckets.all()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step copies images from Labeled Faces in the Wild into your COS bucket.\n",
    "We demonstrate with small data set of 14MB. If you wish to you use entire data set, then use \n",
    "\n",
    "    url = \"http://vis-www.cs.umass.edu/lfw/lfw.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "import io\n",
    " \n",
    "\n",
    "def extractFromStream(url, cos, target_prefix = None):\n",
    "    ftp_stream = urllib.request.urlopen(url)\n",
    "    tarfile_like_object = io.BytesIO(ftp_stream.read())\n",
    "    TarFile_object = tarfile.open(fileobj=tarfile_like_object)\n",
    "    for member in TarFile_object:\n",
    "        if member.isdir() == False:\n",
    "            member_like_object = TarFile_object.extractfile(member)\n",
    "            key = target_prefix\n",
    "            if target_prefix is not None:\n",
    "                key = target_prefix + '/' + member.name\n",
    "            cos.Object(BUCKET, key).put(Body=member_like_object.read())\n",
    "            break\n",
    "\n",
    " \n",
    " \n",
    "url = \"http://vis-www.cs.umass.edu/lfw/lfw-a.tgz\"\n",
    "extractFromStream(url, cos, \"rawimages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bucket_name in buckets:\n",
    "    print(bucket_name)\n",
    "    bucket_obj = cos.Bucket(bucket_name)\n",
    "    for obj in bucket_obj.objects.all():\n",
    "        print(\"  File: {}, {:4.2f}kB\".format(obj.key, obj.size/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data preprocessing with serveless\n",
    "\n",
    "Below, you’ll pre-process the images before passing them into the FaceNet model. Image pre-processing in a facial recognition context typically solves a few problems. These problems range from lighting differences, occlusion, alignment, segmentation. Below, you’ll address segmentation and alignment.\n",
    "First, you’ll solve the segmentation problem by finding the largest face in an image. This is useful as our training data does not have to be cropped for a face ahead of time.\n",
    "Second, you’ll solve alignment. In photographs, it is common for a face to not be perfectly center aligned with the image. To standardize input, you’ll apply a transform to center all images based on the location of eyes and bottom lip.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Detect, Crop & Align with Dlib\n",
    "\n",
    "Upload dlib’s face landmark predictor into your COS bucket. You’ll use this face landmark predictor to find the location of the inner eyes and bottom lips of a face in an image. These coordinates will be used to center align the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\"\n",
    "extractFromStream(url, cos, 'predictor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Preprocessing with IBM Cloud Functions\n",
    "Next, you’ll create a preprocessor for your dataset. This file will read each image into memory, attempt to find the largest face, center align, and write the file to output. If a face cannot be found in the image, logging will be displayed to console with the filename.\n",
    "As each image can be processed independently, python’s multiprocessing is used to process an image on each available cpu core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "import pywren_ibm_cloud as pywren\n",
    "import shutil\n",
    "import cv2\n",
    "\n",
    "from openface.align_dlib import AlignDlib\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "temp_dir = '/tmp'\n",
    "\n",
    "def preprocess_image(bucket, key, data_stream, ibm_cos):\n",
    "    \"\"\"\n",
    "    Detect face, align and crop :param input_path. Write output to :param output_path\n",
    "    :param bucket: COS bucket\n",
    "    :param key: COS key (object name ) - may contain delimiters\n",
    "    :param storage_handler: can be used to read / write data from / into COS\n",
    "    \"\"\"\n",
    "    crop_dim = 180\n",
    "    print(\"Process bucket {} key {}\".format(bucket, key))    \n",
    "    # key of the form /subdir1/../subdirN/file_name\n",
    "    key_components = key.split('/')\n",
    "    file_name = key_components[len(key_components)-1]\n",
    "    input_path = temp_dir + '/' + file_name\n",
    "    if not os.path.exists(temp_dir + '/' + 'output'):\n",
    "        os.makedirs(temp_dir + '/' +'output')\n",
    "    output_path = temp_dir + '/' +'output/'  + file_name\n",
    "    with open(input_path, 'wb') as localfile:\n",
    "        shutil.copyfileobj(data_stream, localfile)\n",
    "    exists = os.path.isfile(temp_dir + '/' +'shape_predictor_68_face_landmarks')\n",
    "    if exists:\n",
    "        pass;\n",
    "    else:\n",
    "        res = ibm_cos.get_object(Bucket = bucket, Key = 'predictor/shape_predictor_68_face_landmarks.dat')\n",
    "        with open(temp_dir + '/' +'shape_predictor_68_face_landmarks', 'wb') as localfile:\n",
    "            shutil.copyfileobj(res['Body'], localfile)\n",
    "    align_dlib = AlignDlib(temp_dir + '/' +'shape_predictor_68_face_landmarks')\n",
    "    image = _process_image(input_path, crop_dim, align_dlib)\n",
    "    if image is not None:\n",
    "        print('Writing processed file: {}'.format(output_path))\n",
    "        cv2.imwrite(output_path, image)\n",
    "        f = open(output_path, \"rb\")\n",
    "        processed_image_path = os.path.join('output',key)\n",
    "        ibm_cos.put_object(Bucket = bucket, Key = processed_image_path, Body = f)\n",
    "        os.remove(output_path)\n",
    "    else:\n",
    "        print(\"Skipping filename: {}\".format(input_path))\n",
    "    os.remove(input_path)\n",
    "\n",
    "def _process_image(filename, crop_dim, align_dlib):\n",
    "    image = None\n",
    "    aligned_image = None\n",
    "    image = _buffer_image(filename)\n",
    "    if image is not None:\n",
    "        aligned_image = _align_image(image, crop_dim, align_dlib)\n",
    "    else:\n",
    "        raise IOError('Error buffering image: {}'.format(filename))\n",
    "    return aligned_image\n",
    "\n",
    "def _buffer_image(filename):\n",
    "    logger.debug('Reading image: {}'.format(filename))\n",
    "    image = cv2.imread(filename, )\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def _align_image(image, crop_dim, align_dlib):\n",
    "    bb = align_dlib.getLargestFaceBoundingBox(image)\n",
    "    aligned = align_dlib.align(crop_dim, image, bb, landmarkIndices=AlignDlib.INNER_EYES_AND_BOTTOM_LIP)\n",
    "    if aligned is not None:\n",
    "        aligned = cv2.cvtColor(aligned, cv2.COLOR_BGR2RGB)\n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Getting Results\n",
    "\n",
    "Now that you’ve created a pipeline, time to get results. As the script supports parallelism, you will see increased performance by running with multiple cores. You’ll need to run the preprocessor in the docker environment to have access to the installed libraries.\n",
    "Below, you’ll mount your project directory as a volume inside the docker container and run the preprocessing script on your input data. The results will be written to a directory specified with command line arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_images = BUCKET + '/rawimages'    \n",
    "pw = pywren.ibm_cf_executor(config=config, runtime='pywren-dlib-runtime_3.5')    \n",
    "pw.map(preprocess_image, raw_images)\n",
    "results = pw.get_result()\n",
    "print(\"Execution completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "\n",
    "Using Dlib, you detected the largest face in an image and aligned the center of the face by the inner eyes and bottom lip. This alignment is a method for standardizing each image for use as feature input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">5. Setup for WML</span>\n",
    "\n",
    "Now that we've preprocessed the data, we’ll generate vector embeddings of each identity. These embeddings can then be used as input to a classification, regression, or clustering task. We will use TensorFlow to create the embeddings and then scikit-learn to create the classifier with these embeddings. However, before we do all this, some preliminary setup is needed.\n",
    "\n",
    "In this section we:\n",
    "\n",
    "- [5.1 Download Pretrained Model](#download-model)\n",
    "- [5.2 Save model files to Cloud Object Storage](#save-model-cos)\n",
    "- [5.3 Authenticate with the WML service instance](#wml-service-instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Download pretrained model<a id=\"download-model\"></a>\n",
    "\n",
    "We will use a pretrained model to simplify the process of generating the embeddings. The model we will use is based off the Inception ResNet V1 architecture and was trained using the <a href=\"http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/\" target=\"_blank\" rel=\"noopener no referrer\">VGGFace2</a> dataset. With the pretrained model and its learned weights, we can use transfer learning in order to create a model that can classify the LFW faces.\n",
    "\n",
    "First, we download and extract the pretrained model using a script copied from <a href=\"https://github.com/davidsandberg/facenet/blob/master/src/download_and_extract.py\" target=\"_blank\" rel=\"noopener no referrer\">here</a>: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "This file is copied and adapted from:\n",
    "https://github.com/davidsandberg/facenet/blob/master/src/download_and_extract.py\n",
    "\"\"\"\n",
    "\n",
    "model_dict = {\n",
    "    '20180402-114759': '1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-'\n",
    "}\n",
    "\n",
    "def download_and_extract_file(model_name, data_dir):\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    file_id = model_dict[model_name]\n",
    "    destination = os.path.join(data_dir, model_name + '.zip')\n",
    "    if not os.path.exists(destination):\n",
    "        print('Downloading file to %s' % destination)\n",
    "        download_file_from_google_drive(file_id, destination)\n",
    "        with zipfile.ZipFile(destination, 'r') as zip_ref:\n",
    "            print('Extracting file to %s' % data_dir)\n",
    "            zip_ref.extractall(data_dir)\n",
    "\n",
    "def download_file_from_google_drive(file_id, destination):\n",
    "\n",
    "        URL = \"https://drive.google.com/uc?export=download\"\n",
    "        session = requests.Session()\n",
    "        response = session.get(URL, params = { 'id' : file_id }, stream = True)\n",
    "        token = get_confirm_token(response)\n",
    "\n",
    "        if token:\n",
    "            params = { 'id' : file_id, 'confirm' : token }\n",
    "            response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "        save_response_content(response, destination)\n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate the download\n",
    "\n",
    "The download is about 184Mb in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_extract_file('20180402-114759', './pretrained-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Save model files to Cloud Object Storage <a id=\"save-model-cos\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have another bucket you want to save the model files to, set it here,\n",
    "# otherwise we will use the same bucket.\n",
    "\n",
    "# BUCKET = ''\n",
    "bucket_obj = cos.Bucket(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "files_search = os.path.join('./pretrained-model/*/*')\n",
    "files = glob.glob(files_search)\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    filename = file.split('/')[-1]\n",
    "    filename = os.path.join(\"pretrained-model\", filename)\n",
    "    print('Uploading data {}...'.format(filename))\n",
    "    bucket_obj.upload_file(file, filename )\n",
    "    print('{} is uploaded.'.format(filename))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Authenticate with the WML service instance <a id=\"wml-service-instance\"></a>\n",
    "\n",
    "Import the libraries you need to work with your WML instance.\n",
    "\n",
    "**Hint**: You may also need to install `wget` using the following command `!pip install wget`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3, requests, json, base64, time, os, wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate to the Watson Machine Learning (WML) service on IBM Cloud.\n",
    "\n",
    "**Tip**: Authentication information (your credentials) can be found in the <a href=\"https://console.bluemix.net/docs/services/service_credentials.html#service_credentials\" target=\"_blank\" rel=\"noopener noreferrer\">Service credentials</a> tab of the service instance that you created on IBM Cloud. \n",
    "If there are no credentials listed for your instance in **Service credentials**, click **New credential (+)** and enter the information required to generate new authentication information. \n",
    "\n",
    "**Action**: Enter your WML service instance credentials here.\n",
    "\n",
    "`\n",
    "wml_credentials = {\n",
    "  \"apikey\": \"------\",\n",
    "  \"iam_apikey_description\": \"------:\",\n",
    "  \"iam_apikey_name\": \"------\",\n",
    "  \"iam_role_crn\": \"-------\",\n",
    "  \"iam_serviceid_crn\": \"-------\",\n",
    "  \"instance_id\": \"-------\",\n",
    "  \"password\": \"------\",\n",
    "  \"url\": \"------\",\n",
    "  \"username\": \"-------\"\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "wml_credentials = {\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the `watson-machine-learning-client` and authenticate to the service instance.\n",
    "\n",
    "**Tip:** If `watson-machine-learning-client` is not preinstalled in your environment, run the following command to install it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install watson-machine-learning-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** A deprecation warning may be returned from scikit-learn package that does not impact watson machine learning client functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = WatsonMachineLearningAPIClient(wml_credentials)\n",
    "print(client.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## <span style=\"color:blue\">6. Create the training definitions</span>\n",
    "\n",
    "With us now connected to our WML service instance, we can now create the training definitions.\n",
    "\n",
    "In this section you:\n",
    "\n",
    "- [6.1 Prepare the training definition metadata](#prep)\n",
    "- [6.2 Get the sample model definition content files from Git](#get)\n",
    "- [6.3 Store the training definition in the WML repository](#store)\n",
    "\n",
    "**Note:** `watson-machine-learning-client` documentation can be found <a href=\"http://wml-api-pyclient.mybluemix.net/\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Prepare the training definition metadata<a id=\"prep\"></a>\n",
    "\n",
    "Prepare the training definition metadata. The main program will use the\n",
    "enviroment variables `$DATA_DIR` and `$RESULT_DIR` in the inputs for the\n",
    "`--model-path`, `--input-dir`, and `--output-path` options.\n",
    "\n",
    "**Tip:** You may want to change the number of epochs to be larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_definition_metadata = {\n",
    "    client.repository.DefinitionMetaNames.NAME: \"TensorFlow Facial Recognition\",\n",
    "    client.repository.DefinitionMetaNames.DESCRIPTION: \"Face Classifier\",\n",
    "    client.repository.DefinitionMetaNames.AUTHOR_NAME: \"IBM Developer\",\n",
    "    client.repository.DefinitionMetaNames.FRAMEWORK_NAME: \"tensorflow\",\n",
    "    client.repository.DefinitionMetaNames.FRAMEWORK_VERSION: \"1.11\",\n",
    "    client.repository.DefinitionMetaNames.RUNTIME_NAME: \"python\",\n",
    "    client.repository.DefinitionMetaNames.RUNTIME_VERSION: \"3.6\",\n",
    "    client.repository.DefinitionMetaNames.EXECUTION_COMMAND: \" \\\n",
    "        python3 train_classifier.py \\\n",
    "            --model-path $DATA_DIR/pretrained-model/20180402-114759.pb \\\n",
    "            --input-dir $DATA_DIR/processed-images \\\n",
    "            --output-path $RESULT_DIR/output-classifier.pkl \\\n",
    "            --num-epochs 3\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Get the sample model definition content file from GitHub <a id=\"get\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='tf-facial-recog.zip'\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "    filename = wget.download('https://github.com/IBM/data-pre-processing-with-pywren/raw/master/data/code/tf-facial-recog.zip')\n",
    "    print(filename, \"was downloaded\")\n",
    "else:\n",
    "    print(filename, \"was downloaded previously.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files in this zip file can be viewed in the GitHub <a href=\"https://github.com/IBM/data-pre-processing-with-pywren/tree/master/data/code\" target=\"_blank\" rel=\"noopener noreferrer\">repository</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Store the training definition in the WML repository<a id=\"store\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_details = client.repository.store_definition(filename, training_definition_metadata)\n",
    "definition_uid = client.repository.get_definition_uid(definition_details)\n",
    "\n",
    "# Display the training definition uid.\n",
    "print(definition_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">7. Train the model</span><a id=\"train\"></a>\n",
    "\n",
    "In this section, learn how to:\n",
    "- [7.1 Enter training configuration metadata](#meta)\n",
    "- [7.2 Train the model in the background](#backg)\n",
    "- [7.3 Monitor the training log](#log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Enter training configuration metadata<a id=\"meta\"></a>\n",
    "\n",
    "- `TRAINING_DATA_REFERENCE` - references the uploaded training data.\n",
    "- `TRAINING_RESULTS_REFERENCE` - location where trained model will be saved.\n",
    "\n",
    "For this exercise, we are going to use the same bucket as the input data to store our resulting model.\n",
    "\n",
    "**Note** Your COS credentials are referenced in this code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the training metadata for the TRAINING_DATA_REFERENCE and TRAINING_RESULTS_REFERENCE.\n",
    "training_configuration_metadata = {\n",
    "    client.training.ConfigurationMetaNames.NAME: \"Face Classifier\", \n",
    "    client.training.ConfigurationMetaNames.AUTHOR_NAME: \"IBM Developer\",              \n",
    "    client.training.ConfigurationMetaNames.DESCRIPTION: \"Training for Face Classifier\",\n",
    "    client.training.ConfigurationMetaNames.COMPUTE_CONFIGURATION: {\"name\": \"k80\"},\n",
    "    client.training.ConfigurationMetaNames.TRAINING_DATA_REFERENCE: {\n",
    "        \"connection\": {\n",
    "            \"endpoint_url\": service_endpoint,\n",
    "            \"access_key_id\": cos_credentials['cos_hmac_keys']['access_key_id'],\n",
    "            \"secret_access_key\": cos_credentials['cos_hmac_keys']['secret_access_key']\n",
    "        },\n",
    "        \"source\": {\n",
    "            \"bucket\": BUCKET,\n",
    "        },\n",
    "        \"type\": \"s3\"\n",
    "    },\n",
    "    client.training.ConfigurationMetaNames.TRAINING_RESULTS_REFERENCE: {\n",
    "        \"connection\": {\n",
    "            \"endpoint_url\": service_endpoint,\n",
    "            \"access_key_id\": cos_credentials['cos_hmac_keys']['access_key_id'],\n",
    "            \"secret_access_key\": cos_credentials['cos_hmac_keys']['secret_access_key']\n",
    "        },\n",
    "        \"target\": {\n",
    "            \"bucket\": BUCKET,\n",
    "        },\n",
    "        \"type\": \"s3\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Train the model in the background<a id=\"backg\"></a>\n",
    "\n",
    "To run the training in the **background**, set the optional parameter `asynchronous=True` (or remove it). In this case the parameter has been removed. \n",
    "\n",
    "**Note:** To run the training in **active** mode, set `asynchronous=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training run.\n",
    "training_run_details = client.training.run(definition_uid, training_configuration_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the power of WML, the embedding creation and subsequent training should be relatively quick.\n",
    "While training, we will be applying additional random transformations and augmentations to the images, boosting our dataset. These images will be fed in a batch size of 128 into the model, and the model will return a 512 dimensional embedding for each image. After these embeddings are created, they will be used as feature inputs into a scikit-learn’s SVM classifier to train on each class. Classes with less than 10 images will be dropped. This parameter is tunable in the execution command specified in the `training_definition_metadata` dictionary above.\n",
    "\n",
    "Get the training run GUID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_run_guid_async = client.training.get_run_uid(training_run_details)\n",
    "print(\"training_run_guid_async=\",training_run_guid_async)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of the training run by calling the following method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training run status.\n",
    "status = client.training.get_status(training_run_guid_async)\n",
    "print(json.dumps(status, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3  Monitor the training log<a id=\"log\"></a>\n",
    "\n",
    "Run the cell below to monitor the training log. This will continue monitoring until the run is finished. If you wish to stop monitoring a current training run, click on the stop button next to \"Run\" button at the top in the notebook options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.training.monitor_logs(training_run_guid_async)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: You can cancel the training run by calling the method `client.training.cancel(training_run_guid_async)`\n",
    "\n",
    "After the training is complete, get the training GUID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_details = client.training.get_details(training_run_guid_async)\n",
    "training_guid = training_details[\"entity\"][\"training_results_reference\"][\"location\"][\"model_location\"]\n",
    "print(\"Training GUID is:\", training_guid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"work\"></a>\n",
    "## <span style=\"color:blue\">8. Work with the Trained Model</span>\n",
    "\n",
    "After the training is complete, the trained model is saved as a file named\n",
    "`output-classifier.pkl` in the result bucket.\n",
    "The following code will fetch the model file from the bucket.\n",
    "\n",
    "**Tip:** Make sure that the training run is completed by checking its\n",
    "status as shown earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_obj = cos.Bucket(BUCKET)\n",
    "\n",
    "# Trained model file name as defined in the code.\n",
    "saved_model_filename = \"output-classifier.pkl\"\n",
    "source_file = os.path.join(training_guid, saved_model_filename)\n",
    "bucket_obj.download_file(source_file,saved_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use model to classify images\n",
    "\n",
    "We are going to use parts of the code that we used with WML to classify images using our trained classifier. Typically we'd want to run the sample image through the same preprocessing steps we used on the training images (face alignment), however we get decent performance just resizing the input images. So let's set that up now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Use the pretrained model we downloaded previously.\n",
    "model_path = './pretrained-model/20180402-114759/20180402-114759.pb'\n",
    "\n",
    "# The trained classifier we downloaded from our COS bucket after training.\n",
    "classifier_path = 'output-classifier.pkl'\n",
    "\n",
    "\n",
    "def run_model(image_paths):\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=False)) as sess:\n",
    "        _load_model(model_filepath=model_path)\n",
    "\n",
    "        dataset = tf.contrib.data.Dataset.from_tensor_slices((image_paths)) \\\n",
    "                .map(_preprocess_function) \\\n",
    "                .batch(128)\n",
    "\n",
    "        init_op = tf.group(tf.global_variables_initializer(),\n",
    "                           tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        batch = iterator.get_next()\n",
    "        batch_images = sess.run(batch)\n",
    "\n",
    "        images_placeholder = \\\n",
    "            tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "        embedding_layer = \\\n",
    "            tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "        phase_train_placeholder = \\\n",
    "            tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "\n",
    "        emb = sess.run(embedding_layer,\n",
    "                feed_dict={images_placeholder: batch_images,\n",
    "                           phase_train_placeholder: False})\n",
    "\n",
    "        with open(classifier_path, 'rb') as f:\n",
    "            model, class_names = pickle.load(f)\n",
    "\n",
    "            predictions = model.predict_proba(emb, )\n",
    "            for index, prediction in enumerate(predictions):\n",
    "                # Display the image in the notebook.\n",
    "                img = Image.open(image_paths[index])\n",
    "                img.thumbnail((100, 100))\n",
    "                display(img)\n",
    "\n",
    "                # Get the indices that would sort the array, then only get the\n",
    "                # indices that correspond to the top 3 predictions.\n",
    "                sorted_indices = prediction.argsort()[::-1][:3]\n",
    "                for index in sorted_indices:\n",
    "                    label = class_names[index]\n",
    "                    confidence = prediction[index]\n",
    "                    print('%s (confidence = %.5f)' % (label, confidence))\n",
    "                print('------------')\n",
    "\n",
    "\n",
    "def _preprocess_function(image_path):\n",
    "    file_contents = tf.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(file_contents, channels=3)\n",
    "    image = tf.image.resize_images([image], (300, 300))[0]\n",
    "    image = tf.image.resize_image_with_crop_or_pad(image, 160, 160)\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def _load_model(model_filepath):\n",
    "    model_exp = os.path.expanduser(model_filepath)\n",
    "    if os.path.isfile(model_exp):\n",
    "        with gfile.FastGFile(model_exp, 'rb') as f:\n",
    "            graph_def = tf.GraphDef()\n",
    "            graph_def.ParseFromString(f.read())\n",
    "            tf.import_graph_def(graph_def, name='')\n",
    "    else:\n",
    "        raise Exception('Specified model %s is not a file.' % (model_exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download some sample images\n",
    "\n",
    "Now, we are going to download some sample images that we can run through the model, so feel free to add other URLs to the list below. Just make sure they are JPEG images. Since we aren't doing the facial alignment preprocessing for this step, pictures where the faces are more prominent work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images_dir = './sample-images'\n",
    "image_urls = [\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/b/b3/Adrien_Brody_Cannes_2017.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0c/SchwarzeneggerJan2010.jpg/800px-SchwarzeneggerJan2010.jpg'       \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "if not os.path.exists(sample_images_dir):\n",
    "    os.makedirs(sample_images_dir)\n",
    "\n",
    "images = []\n",
    "for index, url in enumerate(image_urls):\n",
    "    outfile = os.path.join(sample_images_dir, 'sample{}.jpg'.format(index))\n",
    "    images.append(outfile)\n",
    "    wget.download(url, out=outfile)\n",
    "\n",
    "print(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run classifier on images\n",
    "\n",
    "Now that we downloaded some images, let's try classifying them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## <span style=\"color:blue\"> 9. Summary</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we used PyWren with IBM Cloud Functions to increase preprocessing performance and stored the resulting images in an IBM Cloud Object Storage bucket. From here, we used this bucket along with IBM Watson Machine Learning to create embeddings of each identity using a pretrained TensorFlow FaceNet model, and then create a custom face classifer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "\n",
    "Q. Cao, L. Shen, W. Xie, O. M. Parkhi, A. Zisserman. \"VGGFace2: A dataset for recognising face across pose and age\"  International Conference on Automatic Face and Gesture Recognition, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. <a href=\"https://hackernoon.com/building-a-facial-recognition-pipeline-with-deep-learning-in-tensorflow-66e7645015b8\" target=\"_blank\" rel=\"noopener no referrer\">Building a Facial Recognition Pipeline with Deep Learning in Tensorflow</a> (Original inspiration for code pattern)\n",
    "2. <a href=\"https://github.com/davidsandberg/facenet\" target=\"_blank\" rel=\"noopener no referrer\">Face Recognition using Tensorflow</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    "\n",
    "**Gil Vernik?**\n",
    "\n",
    "**Paul Van Eck**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2019 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n",
    "<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n",
    "<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n",
    "<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
